{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"./images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
        "\n",
        "# ML through Application \n",
        "## Module 3, Lab 2, Notebook 2: Exploring Model Predictions for Bias\n",
        "\n",
        "This notebook shows you how to build a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model to predict whether an individual's income is $\\leq$ 50k by using data from the U.S. Census Bureau.\n",
        "\n",
        "You will learn how to do the following:\n",
        "- Build a simple classifier by using a scikit-learn pipeline and ColumnTransformer.\n",
        "- Evaluate the classifier by using fairness metrics.\n",
        "- Compare bias in data before training and bias in the model predictions.\n",
        "\n",
        "__Dataset:__ \n",
        "You will use [Folktables](https://github.com/zykls/folktables) to download a dataset for this lab. Folktables provides an API to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files, which the U.S. Census Bureau manages. The data itself is governed by the terms of use that are provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html). \n",
        "\n",
        "You will filter the ACS PUMS data sample to include only individuals who are above the age of 16, reported usual working hours of at least 1 hour per week in the past year, and have an income of at least \\\\$100. \n",
        "The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable replacement to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult), but the income threshold can be changed easily to define new prediction tasks. Historically, the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult) served as the basis for the development and comparison of many algorithmic fairness interventions but has limited documentation, outdated feature encodings, and only contains a binary target label which can lead to misrepresentations for certain subpopulations. In order to compare your results with scientific findings that utilize the UCI Adult dataset, and to have greater control and flexibility in setting up the problem, you will utilize the ACS PUMS data with the filters and thresholds described above.\n",
        "\n",
        "__ML problem:__ \n",
        "The goal is to predict whether an individual's income is above \\\\$50,000. \n",
        "This is a binary prediction task that can enable organizations and businesses to target their marketing efforts more effectively. Alternatively, governments could leverage these predictions to design better social welfare programs and allocate resources efficiently. Keep these kinds of problems in mind, when working through the notebook.\n",
        "\n",
        "Reference: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science.\n",
        "\n",
        "----\n",
        "\n",
        "You will be presented with activities throughout the notebook: <br/>\n",
        "\n",
        "| <img style=\"float: center;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>| \n",
        "| --- | \n",
        "|<p style=\"text-align:center;\"> No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p>|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index\n",
        "\n",
        "- [Read in the dataset](#Read-in-the-dataset)\n",
        "- [Data processing](#Data-processing)\n",
        "- [Train a classifier](#Train-a-classifier)\n",
        "- [Test the classifier](#Test-the-classifier)\n",
        "- [Calculate DPPL and accuracy difference](#Calculate-DPPL-and-accuracy-difference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before reading in the dataset, make sure to install and import all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Use pip to install libraries\n",
        "!pip install --no-deps -U -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import the libraries needed for the notebook\n",
        "\n",
        "# Reshaping/basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Operational libraries\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "sys.path.insert(1, \"..\")\n",
        "from MLUMLA_EN_M3_Lab2_quiz_questions import *\n",
        "\n",
        "# Fairness libraries\n",
        "from folktables.acs import *\n",
        "from folktables.folktables import *\n",
        "from folktables.load_acs import *\n",
        "\n",
        "# Jupyter(lab) libraries\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Read in the dataset\n",
        "\n",
        "Import the data from Folktables the same way you did in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "income_features = [\n",
        "    \"AGEP\",  # age individual\n",
        "    \"COW\",  # class of worker\n",
        "    \"SCHL\",  # educational attainment\n",
        "    \"MAR\",  # marital status\n",
        "    \"OCCP\",  # occupation\n",
        "    \"POBP\",  # place of birth\n",
        "    \"RELP\",  # relationship\n",
        "    \"WKHP\",  # hours worked per week past 12 months\n",
        "    \"SEX\",  # sex\n",
        "    \"RAC1P\",  # recorded detailed race code\n",
        "    \"PWGTP\",  # persons weight\n",
        "    \"GCL\",  # grandparents living with grandchildren\n",
        "]\n",
        "\n",
        "# Define the prediction problem and features\n",
        "ACSIncome = folktables.BasicProblem(\n",
        "    features=income_features,\n",
        "    target=\"PINCP\",  # total persons income\n",
        "    target_transform=lambda x: x > 50000,\n",
        "    group=\"RAC1P\",\n",
        "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
        "    postprocess=lambda x: x,  # applies post processing, for example: fill all NAs\n",
        ")\n",
        "\n",
        "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
        "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
        "# Specify region (here: California) and load data\n",
        "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
        "# Apply transformation as per problem statement above\n",
        "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform basic data processing by creating a dataframe with the features and then filter the dataframe so it only contains data from groups 6 and 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Convert NumPy array to DataFrame\n",
        "df = pd.DataFrame(\n",
        "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
        "    columns=income_features + [\">50k\"],\n",
        ")\n",
        "\n",
        "# For further modeling, use only two groups\n",
        "df = df[df[\"RAC1P\"].isin([6, 8])].copy(deep=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Data processing\n",
        "\n",
        "### EDA\n",
        "\n",
        "Look at the number of rows, number of columns, and some simple statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Print the first five rows\n",
        "# NaN means missing data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check how many rows and columns are in the DataFrame\n",
        "print(\"The shape of the dataset is:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# See the data types and non-null values for each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that all columns are numerical (`dtype = float64`). However, check the column headers (and information about the dataset from the links given at the beginning of the notebook), and notice that you are actually dealing with multimodal data. The dataset has a mix of categorical, numerical, and potentially even text information.\n",
        "\n",
        "Cast the features accordingly. You start by creating a list for each feature type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "categorical_features = [\n",
        "    \"COW\",\n",
        "    \"SCHL\",\n",
        "    \"MAR\",\n",
        "    \"OCCP\",\n",
        "    \"POBP\",\n",
        "    \"RELP\",\n",
        "    \"SEX\",\n",
        "    \"RAC1P\",\n",
        "    \"GCL\",\n",
        "]\n",
        "\n",
        "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Cast categorical features to `category`\n",
        "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
        "\n",
        "# Cast numerical features to `int`\n",
        "df[numerical_features] = df[numerical_features].astype(\"int\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make sure that the changes took effect, use `.info()` to check. Compare the results to the `df.info()` output above to make sure the features were cast correctly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can now separate model features from the model target to explore them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_target = \">50k\"\n",
        "model_features = categorical_features + numerical_features\n",
        "\n",
        "print(\"Model features: \", model_features)\n",
        "print(\"Model target: \", model_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check that the target is not accidentally part of the features\n",
        "model_target in model_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks good. You made sure that the target is not in the feature list. If the output of the previous cell is `True`, you need to remove the target by calling `model_features.remove(model_target)`.\n",
        "\n",
        "Next, you will look at missing values.\n",
        "\n",
        "#### Check for missing values\n",
        "\n",
        "The quickest way to check for missing values is to use `.isna().sum()`. This will provide a count of missing values.\n",
        "\n",
        "You can also see the count of missing values with `.info()` because the function provides a count of non-null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you find a column where the majority of the values are missing, consider to exclude this column from the feature list. Alternatively, you can find a way to fill the missing values and impute them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting to create plots, look at how many unique instances you have per column. This helps you avoid plotting charts with hundreds of unique values. To do this, filter for columns with fewer than 10 unique instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "shortlist_fts = (\n",
        "    df[model_features]\n",
        "    .apply(lambda col: col.nunique())\n",
        "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
        "    .dropna()\n",
        ")\n",
        "\n",
        "print(shortlist_fts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check the target distribution\n",
        "\n",
        "Check the target distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df[model_target].value_counts().plot.bar(color=\"black\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the dataset is imbalanced. This means that the dataset has more examples for one type of result (here: 0; meaning individuals that earn $\\leq$ 50k). This is relevant for model choice and potential upsampling or downsampling to balance out the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create feature distributions\n",
        "\n",
        "Now, plot bar charts for the shortlist features (feature columns with fewer than 10 unique instance classes) of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "fig.suptitle(\"Feature Bar Plots\")\n",
        "\n",
        "fts = range(len(shortlist_fts.index.tolist()))\n",
        "for i, ax in zip(fts, axs.ravel()):\n",
        "    df[shortlist_fts.index.tolist()[i]].value_counts().plot.bar(color=\"black\", ax=ax)\n",
        "    ax.set_title(shortlist_fts.index.tolist()[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Select features to build the model \n",
        "\n",
        "The GCL feature is equally present in both outcome instances and also contains a lot of missing values. Therefore, you can drop it from the list of features that you want to use for model build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "to_remove = \"GCL\"\n",
        "\n",
        "# Drop GCL feature from the respective list(s) - if applicable\n",
        "if to_remove in model_features:\n",
        "    model_features.remove(to_remove)\n",
        "if to_remove in categorical_features:\n",
        "    categorical_features.remove(to_remove)\n",
        "if to_remove in numerical_features:\n",
        "    numerical_features.remove(to_remove)\n",
        "\n",
        "# Clean up the DataFrame and only keep the features and columns that are needed\n",
        "df = df[model_features + [model_target]].copy(deep=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create training, test, and validation datasets \n",
        "\n",
        "To get training, test, and validation sets, use scikit-learn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(\n",
        "    df, test_size=0.1, shuffle=True, random_state=23\n",
        ")\n",
        "train_data, val_data = train_test_split(\n",
        "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\n",
        "    \"Train - Test - Validation dataset shapes: \",\n",
        "    train_data.shape,\n",
        "    test_data.shape,\n",
        "    val_data.shape,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the data with a pipeline and ColumnTransformer \n",
        "\n",
        "Now you can build a full model pipeline. You need a preprocessing split per data type. Then, you will combine everything into a composite pipeline along with a model. To achieve this, you will use scikit-learn's `Pipeline` and `ColumnTransformer`.\n",
        "\n",
        "__Step 1: Set up preprocessing per data type__\n",
        "\n",
        "For the numerical features pipeline (`numerical_processor` in the following code cell), impute missing values with the mean by using scikit-learn's `SimpleImputer`. Then, use `MinMaxScaler` (you don't need to scale features when using decision trees, but it's a good idea to see how to use data transforms). If different processing is desired for different numerical features, you should build different pipelines, as shown for the two categorical text features.\n",
        "\n",
        "In the categorical features pipeline, (`categorical_processor` in the following code cell), impute with a placeholder value and encode with scikit-learn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check unique values for categoricals to get an estimate of how many dummy features will be created by one-hot encoding. Note the `handle_unknown` parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set.\n",
        " \n",
        "__Step 2: Combine preprocessing methods into a transformer__ \n",
        "\n",
        "The selective preparations of the dataset features are then put together into a collective `ColumnTransformer` to finally be used in a pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset through cross-validation or making predictions on a test dataset in the future.\n",
        "   \n",
        "__Step 3: Combine the transformer with a model__ \n",
        "\n",
        "Combine `ColumnTransformer` from step 2 with a selected algorithm in a new pipeline. For example, the algorithm could be a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "### STEP 1 ###\n",
        "##############\n",
        "\n",
        "# Preprocess the numerical features\n",
        "numerical_processor = Pipeline(\n",
        "    [(\"num_imputer\", SimpleImputer(strategy=\"mean\")), (\"num_scaler\", MinMaxScaler())]\n",
        ")\n",
        "# Preprocess the categorical features\n",
        "categorical_processor = Pipeline(\n",
        "    [\n",
        "        (\"cat_imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### STEP 2 ###\n",
        "##############\n",
        "\n",
        "# Combine all data preprocessors from step 1\n",
        "data_processor = ColumnTransformer(\n",
        "    [\n",
        "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
        "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### STEP 3 ###\n",
        "##############\n",
        "\n",
        "# Pipeline desired all data transformers, along with an estimator at the end\n",
        "# Later you can set or reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\"data_processing\", data_processor),\n",
        "        (\"lg\", LogisticRegression(solver=\"liblinear\", penalty=\"l2\", C=0.001)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Visualize the pipeline\n",
        "# This will be helpful, especially when building more complex pipelines, stringing together multiple preprocessing steps\n",
        "from sklearn import set_config\n",
        "\n",
        "set_config(display=\"diagram\")\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Train a classifier \n",
        "\n",
        "Use the pipeline with the desired data transformers, along with a logistic regression estimator for training.\n",
        "\n",
        "Train the classifier with `.fit()` on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get training data to train the classifier\n",
        "X_train = train_data[model_features]\n",
        "y_train = train_data[model_target]\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "# Training data going through the pipeline is imputed (with means from the training data),\n",
        "#   scaled (with the min/max from the training data),\n",
        "#   and finally used to fit the model\n",
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get validation data to tune the classifier\n",
        "X_val = val_data[model_features]\n",
        "y_val = val_data[model_target]\n",
        "\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "print(\"Model performance on the validation set:\")\n",
        "print(\"Validation accuracy:\", accuracy_score(y_val, y_val_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test the classifier \n",
        "\n",
        "Now, evaluate the performance of the trained classifier on the test dataset by using `.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get test data to validate the classifier\n",
        "X_test = test_data[model_features]\n",
        "y_test = test_data[model_target]\n",
        "\n",
        "# Use the fitted model to make predictions on the test dataset\n",
        "# Test data going through the pipeline is imputed (with means from the training data),\n",
        "#   scaled (with the min/max from the training data),\n",
        "#   and finally used to make predictions\n",
        "test_predictions = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Model performance on the test set:\")\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Calculate DPPL and accuracy difference\n",
        "\n",
        "### DPPL\n",
        "\n",
        "Difference in positive proportions in predicted labels (DPPL) is an extension of DPL where the outcome that you compare is the prediction that the model creates (rather than the ground truth value). The equation remains basically the same, but you count positive outcomes as per model prediction:\n",
        "\n",
        "$\\LARGE DPPL = \\frac{\\hat{n}_{pred>50k \\wedge RAC1P=6}}{n_{RAC1P=6}} - \\frac{\\hat{n}_{pred>50k \\wedge RAC1P=8}}{n_{RAC1P=8}}$\n",
        "\n",
        "To calculate DPPL more easily, write a function that can take different parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dppl(sensitive_attribute_name, attr_val, target, dataframe):\n",
        "    \"\"\"Function to calculate DPL or DPPL (depending on specified target).\"\"\"\n",
        "    for val in attr_val:\n",
        "        globals()[f\"n_pos_gr{val}\"] = len(\n",
        "            dataframe[\n",
        "                (dataframe[target] == 1) & (dataframe[sensitive_attribute_name] == val)\n",
        "            ]\n",
        "        )\n",
        "        globals()[f\"n_gr{val}\"] = len(\n",
        "            dataframe[dataframe[sensitive_attribute_name] == val]\n",
        "        )\n",
        "\n",
        "    dpl = n_pos_gr6 / n_gr6 - n_pos_gr8 / n_gr8\n",
        "    return dpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame that contains predictions and the sensitive attribute\n",
        "dpl_df = pd.concat(\n",
        "    [\n",
        "        test_data.reset_index(drop=True)[[\"RAC1P\", \">50k\"]],\n",
        "        pd.Series(test_predictions, name=\"y_test_pred\"),\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "dppl(\"RAC1P\", [6, 8], \"y_test_pred\", dpl_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, compare this to the original DPL value (the difference in proportion of labels in the original DataFrame)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\"> \n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">How can you use the DPPL function to check for difference in proportion of labels before the model was trained?</p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To answer the question, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell for a knowledge check question\n",
        "question_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you calculate the DPL value, the results will show that it is smaller than the DPPL value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dppl(\"RAC1P\", [6, 8], \">50k\", test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means that the model produces a more biased output than the ground truth data. This could be because of different success rates for the different groups in the model. Generally, models will make better predictions for the larger group (meaning that the larger group drives the overall model performance, and whatever outcome is dominant in the larger group will occur more frequently in the predictions).\n",
        "\n",
        "See if you observe this behavior in the model predictions. First, calculate overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy score across all groups\n",
        "accuracy_score(dpl_df[\">50k\"], dpl_df[\"y_test_pred\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Now, look at the accuracy difference between `RACP1=6` and `RACP1=8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy score for RACP1=6\n",
        "acc_gr6 = accuracy_score(\n",
        "    dpl_df[dpl_df[\"RAC1P\"] == 6][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 6][\"y_test_pred\"]\n",
        ")\n",
        "\n",
        "acc_gr6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy score for RACP1=8\n",
        "acc_gr8 = accuracy_score(\n",
        "    dpl_df[dpl_df[\"RAC1P\"] == 8][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 8][\"y_test_pred\"]\n",
        ")\n",
        "\n",
        "acc_gr8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy difference\n",
        "\n",
        "To calculate the accuracy difference, calculate the difference between the two accuracy scores that you just calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\"> \n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">How can you calculate the accuracy difference between group 6 and group 8?</p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To answer the question, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell for a knowledge check question\n",
        "question_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the accuracy for the group that you assume to be at a disadvantage is higher than the accuracy for the favored group.\n",
        "\n",
        "Dive deeper to see what's going on. Output the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for RACP1=8\n",
        "confusion_matrix(\n",
        "    dpl_df[dpl_df[\"RAC1P\"] == 8][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 8][\"y_test_pred\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, compare this confusion matrix with the one for the favored group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for RACP1=6\n",
        "confusion_matrix(\n",
        "    dpl_df[dpl_df[\"RAC1P\"] == 6][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 6][\"y_test_pred\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The errors are not balanced between false positives and false negatives for the disfavored group. There is a much larger share of false negatives (predicted salary as $\\leq$ 50k, when it was actually higher) than false positives.\n",
        "\n",
        "This result is concerning for multiple reasons:\n",
        "- It gives the impression that the model undervalues the disfavored group (the group has more $\\leq$ 50k salaries than there are in reality).\n",
        "- If this model is used to show job ads based on a 50k salary threshold, a lot of individuals in the disfavored group that should have been eligible to see the ad won't receive it. This causes a negative reinforcement of existing bias.\n",
        "\n",
        "You can see this when you compare plots of the model prediction and the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.catplot(x=\"RAC1P\", kind=\"count\", hue=model_target, data=dpl_df, palette=\"husl\")\n",
        "plt.ylim(0, 2500)\n",
        "sns.catplot(x=\"RAC1P\", kind=\"count\", hue=\"y_test_pred\", data=dpl_df, palette=\"husl\")\n",
        "plt.ylim(0, 2500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Conclusion\n",
        "\n",
        "The imbalance is not as drastic for the favored group where the predictions and ground truth bar charts look  similar. However, for the disfavored group, the predicted class is almost exclusively negative. If this model were put in production, group 8 would be at a further disadvantage.\n",
        "\n",
        "Also, notice how the overall model performance (as measured in accuracy) is relatively high. This is because the outcomes are imbalanced with a lot more negative examples in total. Remember that this model was trained with the sensitive feature as part of the model features. You could try removing this to see if that leads to better outcomes for the disfavored group."
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
