{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"./images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
        "\n",
        "# ML through Application \n",
        "## Module 3, Lab 3: Implementing a DI Remover\n",
        "\n",
        "This notebook shows you how to quantify disparate impact (DI) and implement a basic DI remover. You will use a logistic regression model to predict whether an individual's income is $\\leq$ 50k by using data from the U.S. Census Bureau.\n",
        "\n",
        "You will learn how to do the following:\n",
        "- Transform a dataset by using a DI remover.\n",
        "- Measure DI before and after model training.\n",
        "\n",
        "__Dataset:__ \n",
        "You will use [Folktables](https://github.com/zykls/folktables) to download a dataset for this lab. Folktables provides an API to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files, which the U.S. Census Bureau manages. The data itself is governed by the terms of use that are provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html). \n",
        "\n",
        "You will filter the ACS PUMS data sample to include only individuals who are above the age of 16, reported usual working hours of at least 1 hour per week in the past year, and have an income of at least \\\\$100. \n",
        "The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable replacement to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult), but the income threshold can be changed easily to define new prediction tasks. Historically, the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult) served as the basis for the development and comparison of many algorithmic fairness interventions but has limited documentation, outdated feature encodings, and only contains a binary target label which can lead to misrepresentations for certain subpopulations. In order to compare your results with scientific findings that utilize the UCI Adult dataset, and to have greater control and flexibility in setting up the problem, you will utilize the ACS PUMS data with the filters and thresholds described above.\n",
        "\n",
        "__ML problem:__ \n",
        "The goal is to predict whether an individual's income is above \\\\$50,000. \n",
        "This is a binary prediction task that can enable organizations and businesses to target their marketing efforts more effectively. Alternatively, governments could leverage these predictions to design better social welfare programs and allocate resources efficiently. Keep these kinds of problems in mind, when working through the notebook.\n",
        "\n",
        "Reference: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science.\n",
        "\n",
        "----\n",
        "\n",
        "You will be presented with activities throughout the notebook: <br/>\n",
        "\n",
        "| <img style=\"float: center;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>| \n",
        "| --- | \n",
        "|<p style=\"text-align:center;\"> No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p>|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index\n",
        "\n",
        "- [Read in the dataset](#Read-in-the-dataset)\n",
        "- [Data processing](#Data-processing)\n",
        "- [Train a classifier](#Train-a-classifier)\n",
        "- [Test the classifier](#Test-the-classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before loading in the dataset, make sure to install and import all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Use pip to install libraries\n",
        "!pip install --no-deps -U -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Import the libraries needed for the notebook\n",
        "\n",
        "# Reshaping/basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Operational libraries\n",
        "from MLUMLA_EN_M3_Lab3_quiz_questions import *\n",
        "from EqualityOfOddsJupy import *\n",
        "\n",
        "# Fairness libraries\n",
        "from folktables.acs import *\n",
        "from folktables.folktables import *\n",
        "from folktables.load_acs import *\n",
        "from aif360.datasets import BinaryLabelDataset, Dataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "\n",
        "# Jupyter(lab) libraries\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Read in the dataset\n",
        "\n",
        "Import the data from Folktables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "income_features = [\n",
        "    \"AGEP\",  # age individual\n",
        "    \"COW\",  # class of worker\n",
        "    \"SCHL\",  # educational attainment\n",
        "    \"MAR\",  # marital status\n",
        "    \"OCCP\",  # occupation\n",
        "    \"POBP\",  # place of birth\n",
        "    \"RELP\",  # relationship\n",
        "    \"WKHP\",  # hours worked per week past 12 months\n",
        "    \"SEX\",  # sex\n",
        "    \"RAC1P\",  # recorded detailed race code\n",
        "    \"PWGTP\",  # persons weight\n",
        "    \"GCL\",  # grandparents living with grandchildren\n",
        "]\n",
        "\n",
        "# Define the prediction problem and features\n",
        "ACSIncome = folktables.BasicProblem(\n",
        "    features=income_features,\n",
        "    target=\"PINCP\",  # total persons income\n",
        "    target_transform=lambda x: x > 50000,\n",
        "    group=\"RAC1P\",\n",
        "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
        "    postprocess=lambda x: x,  #  # applies post processing, for example: fill all NAs\n",
        ")\n",
        "\n",
        "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
        "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
        "# Specify region (here: California) and load data\n",
        "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
        "# Apply transformation as per problem statement above\n",
        "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)\n",
        "\n",
        "# Convert NumPy array to DataFrame\n",
        "df = pd.DataFrame(\n",
        "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
        "    columns=income_features + [\">50k\"],\n",
        ")\n",
        "\n",
        "# For further modeling, use only two groups\n",
        "df = df[df[\"RAC1P\"].isin([6, 8])].copy(deep=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Data processing\n",
        "\n",
        "### EDA\n",
        "\n",
        "Look at the number of rows, number of columns, and some simple statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Print the first five rows\n",
        "# NaN means missing data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check how many rows and columns are in the DataFrame\n",
        "print(\"The shape of the dataset is:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# See the data types and non-null values for each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that all columns are numerical (`dtype = float64`). However, check the column headers (and information about the dataset from the links given at the beginning of the notebook), and notice that you are actually dealing with multimodal data. The dataset has a mix of categorical, numerical, and potentially even text information.\n",
        "\n",
        "You start by creating a list for each feature type. To keep things simple, you should start by only using one feature as sensitive attribute. Cast all other the features according to the desired data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "categorical_features = [\n",
        "    \"COW\",\n",
        "    \"SCHL\",\n",
        "    \"MAR\",\n",
        "    \"OCCP\",\n",
        "    \"POBP\",\n",
        "    \"RELP\",\n",
        "    \"SEX\",\n",
        "    \"GCL\",\n",
        "]\n",
        "\n",
        "sensitive_attribute = \"RAC1P\"\n",
        "\n",
        "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Cast categorical features to `category`\n",
        "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
        "\n",
        "# Cast the sensitive attribute as `category`\n",
        "df[sensitive_attribute] = df[sensitive_attribute].astype(\"object\")\n",
        "\n",
        "# Cast numerical features to `int`\n",
        "df[numerical_features] = df[numerical_features].astype(\"int\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make sure that the changes took effect, use `.info()` to check. Compare the results to the `df.info()` output above to make sure the features were cast correctly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can now separate model features from the model target to explore them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_target = \">50k\"\n",
        "model_features = categorical_features + numerical_features + [sensitive_attribute]\n",
        "\n",
        "print(\"Model features: \", model_features)\n",
        "print(\"Model target: \", model_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check that the target is not accidentally part of the features\n",
        "model_target in model_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "This looks good. You made sure that the target is not in the feature list. If the output of the previous cell is `True`, you need to remove the target by calling `model_features.remove(model_target)`.\n",
        "\n",
        "Next, you will look for missing values.\n",
        "\n",
        "#### Check for missing values\n",
        "\n",
        "The quickest way to check for missing values is to use `.isna().sum()`. This will provide a count of missing values.\n",
        "\n",
        "You can also see the count of missing values with `.info()` because the function provides a count of non-null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you find a column where the majority of the values are missing, consider to exclude this column from the feature list. Alternatively, you can find a way to fill the missing values and impute them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting to create plots, look at how many unique instances you have per column. This helps you avoid plotting charts with hundreds of unique values. To do this, filter for columns with fewer than 10 unique instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "shortlist_fts = (\n",
        "    df[model_features]\n",
        "    .apply(lambda col: col.nunique())\n",
        "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
        "    .dropna()\n",
        ")\n",
        "\n",
        "print(shortlist_fts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check the target distribution\n",
        "\n",
        "Check the target distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df[model_target].value_counts().plot.bar(color=\"black\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the dataset is imbalanced. This means that the dataset has more examples for one type of result (here: 0; meaning individuals that earn $\\leq$ 50k). This is relevant for model choice and potential upsampling or downsampling to balance out the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create feature distributions\n",
        "\n",
        "Now, plot bar charts for the shortlist features (feature columns with fewer than 10 unique instance classes) of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "fig.suptitle(\"Feature Bar Plots\")\n",
        "\n",
        "fts = range(len(shortlist_fts.index.tolist()))\n",
        "for i, ax in zip(fts, axs.ravel()):\n",
        "    df[shortlist_fts.index.tolist()[i]].value_counts().plot.bar(color=\"black\", ax=ax)\n",
        "    ax.set_title(shortlist_fts.index.tolist()[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Select features to build the model \n",
        "\n",
        "The GCL feature is equally present in both outcome instances and also contains a lot of missing values. Therefore, you can drop it from the list of features that you want to use for model build just like you dropped OCCP and POBP since those features had more than 10 unique categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "to_remove = [\"GCL\", \"OCCP\", \"POBP\"]\n",
        "\n",
        "# Drop to_remove features from the respective list(s) - if applicable\n",
        "for ft in to_remove:\n",
        "    if ft in model_features:\n",
        "        model_features.remove(ft)\n",
        "    if ft in categorical_features:\n",
        "        categorical_features.remove(ft)\n",
        "    if ft in numerical_features:\n",
        "        numerical_features.remove(ft)\n",
        "\n",
        "# Clean up the DataFrame and only keep the features and columns that are needed\n",
        "df = df[model_features + [model_target]].copy(deep=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature transformation\n",
        "\n",
        "In the paper [Certifying and Removing Disparate Impact](https://arxiv.org/pdf/1412.3756.pdf), a definition for disparate impact (DI) was introduced as the ratio of probability of positive outcomes for the disfavored group $(A=0)$ to the probability of positive outcomes for the favored group $(A=1)$:  \n",
        "\n",
        "$\\Large DI = \\frac{Pr(Y=y|A=0)}{Pr(Y=y|A=1)}$\n",
        "\n",
        "To be free of DI, the resulting value from the equation needs to be $>$0.8 (and as close to 1 as possible). However, achieving this can be difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following code cell, and look at the interactive visualization to compare two groups: triangles and circles. This example has a positive outcome (accepted) and a negative outcome (rejected). You might notice that the triangle group is favored because more than 50 percent of the triangles have historical records (truth) for \"accepted. At this point, you can calculate the DI by counting how many positive outcomes a group has divided by the group total. This will confirm that the triangle group is favored.\n",
        "\n",
        "Next, move the slider (which represents a classifier threshold) on the first plot to see if a threshold value exists where the probabilities to receive a positive (or negative) outcome are equal across groups.\n",
        "\n",
        "Notice that it's impossible to find one position where the probabilities for both groups are the same. This makes sense because the distributions of accepted/rejected differ by group. Therefore, a single threshold will not work equally well for both groups.\n",
        "\n",
        "At this point, you have different options, including the following:\n",
        "- Transform the data so that the distributions of the groups look more similar (DI remover).\n",
        "- Train a classifier and use different probability thresholds.\n",
        "\n",
        "In this notebook, you will see the transformation of data (DI remover). In another lab, you will see a postprocessing technique that uses different probability thresholds for different groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "EqualityOfOddsJupy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create training, test, and validation datasets \n",
        "\n",
        "To get training, test, and validation sets, use scikit-learn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(\n",
        "    df, test_size=0.1, shuffle=True, random_state=23\n",
        ")\n",
        "\n",
        "train_data, val_data = train_test_split(\n",
        "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\n",
        "    \"Train - Test - Validation dataset shapes: \",\n",
        "    train_data.shape,\n",
        "    test_data.shape,\n",
        "    val_data.shape,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the data with a pipeline and ColumnTransformer \n",
        "\n",
        "Next, you will build a data processing pipeline. You need a preprocessing split per data type. Then, you will combine everything into a composite pipeline. To achieve this, you will use scikit-learn's `Pipeline` and `ColumnTransformer`.\n",
        "\n",
        "__Step 1: Set up preprocessing per data type__\n",
        "\n",
        "For the numerical features pipeline (`numerical_processor` in the following code cell), impute missing values with the mean by using scikit-learn's `SimpleImputer`. Then, use `MinMaxScaler` (you don't need to scale features when using decision trees, but it's a good idea to see how to use data transforms). If different processing is desired for different numerical features, you should build different pipelines, as shown for the two categorical text features.\n",
        "\n",
        "In the categorical features pipeline, (`categorical_processor` in the following code cell), impute with a placeholder value and encode with scikit-learn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check unique values for categoricals to get an estimate of how many dummy features will be created by one-hot encoding. Note the `handle_unknown` parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set.\n",
        "\n",
        "__Step 2: Combine preprocessing methods into a transformer__ \n",
        "\n",
        "The selective preparations of the dataset features are then put together into a collective `ColumnTransformer` to finally be used in a pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset through cross-validation or making predictions on a test dataset in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### STEP 1 ###\n",
        "##############\n",
        "\n",
        "# Preprocess the numerical features\n",
        "numerical_processor = Pipeline(\n",
        "    [(\"num_imputer\", SimpleImputer(strategy=\"mean\")), (\"num_scaler\", MinMaxScaler())]\n",
        ")\n",
        "# Preprocess the categorical features\n",
        "categorical_processor = Pipeline(\n",
        "    [\n",
        "        (\"cat_imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### STEP 2 ###\n",
        "##############\n",
        "\n",
        "# Combine all data preprocessors from step 1\n",
        "data_processor = ColumnTransformer(\n",
        "    [\n",
        "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
        "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the transformation before performing DI removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Learn the transformation and extract the feature names\n",
        "data_processor.fit(train_data)\n",
        "\n",
        "# To extract feature names, first fit the data processor because\n",
        "# this will generate the one-hot encoding\n",
        "ft_names = numerical_features + list(\n",
        "    data_processor.transformers_[1][1]\n",
        "    .named_steps[\"cat_encoder\"]\n",
        "    .get_feature_names_out(categorical_features)\n",
        ")\n",
        "\n",
        "train_prep = data_processor.transform(train_data).todense()\n",
        "\n",
        "prep = np.concatenate(\n",
        "    (\n",
        "        train_prep,\n",
        "        train_data[[model_target]].values,\n",
        "        train_data[[sensitive_attribute]].values,\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "# Add column names and convert to a DataFrame\n",
        "prep_df = pd.DataFrame(prep, columns=ft_names + [model_target] + [sensitive_attribute])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the DI transformation, you need to create an Artificial Intelligence Fairness (AIF360) dataset using the AIF360 library. You can do this by using `BinaryLabelDataset`. Then, you can use `DisparateImpactRemover`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset construct for AIF360\n",
        "binaryLabelDataset = BinaryLabelDataset(\n",
        "    df=prep_df,\n",
        "    label_names=[\">50k\"],\n",
        "    protected_attribute_names=[\"RAC1P\"],\n",
        "    favorable_label=1.0,\n",
        "    unfavorable_label=0.0,\n",
        ")\n",
        "\n",
        "# Use DisparateImpactRemover at a medium repair level\n",
        "di_remover = DisparateImpactRemover(repair_level=0.9)\n",
        "\n",
        "# Perform the transformation\n",
        "binaryLabelDataset_transform = di_remover.fit_transform(binaryLabelDataset)\n",
        "\n",
        "# Save as a DataFrame\n",
        "df_transform = binaryLabelDataset_transform.convert_to_dataframe()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first rows of the transformed dataset\n",
        "df_transform.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For easier readability, show the original DataFrame so that you can compare directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first rows of the orignal dataset\n",
        "prep_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, look at the transformed dataset. You can see that certain values were adjusted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a final check, you can also calculate the DI value. To do this, you need to specify which group is favored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Declare what the attribute value of the (un)privileged group is\n",
        "priv_group = [{\"RAC1P\": 6}]\n",
        "unpriv_group = [{\"RAC1P\": 8}]\n",
        "\n",
        "# Find out which index the sensitive attribute has to delete later\n",
        "sensitive_attribute_index = binaryLabelDataset_transform.feature_names.index(\n",
        "    sensitive_attribute\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, calculate the DI value with the AIF360 inbuilt method `BinaryLabelDatasetMetric()`. You need to pass the DataFrame into the metric, and then you can calculate DI by using `.disparate_impact()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the DI value\n",
        "print(\n",
        "    BinaryLabelDatasetMetric(\n",
        "        binaryLabelDataset,\n",
        "        unprivileged_groups=unpriv_group,\n",
        "        privileged_groups=priv_group,\n",
        "    ).disparate_impact()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how to calculate the DI value for the transformed dataset, run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BinaryLabelDatasetMetric(\n",
        "        binaryLabelDataset_transform,\n",
        "        unprivileged_groups=unpriv_group,\n",
        "        privileged_groups=priv_group,\n",
        "    ).disparate_impact()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The value shouldn't change because the remover does not touch the labels. The goal of the DI remover is to shift the distribution of the nonsensitive features while preserving the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Train a classifier\n",
        "\n",
        "Use the transformed dataset and a logistic regression estimator for training.\n",
        "\n",
        "Train the classifier with `.fit()` on the repaired training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features (and delete the sensitive attribute) and label\n",
        "X_train = np.delete(\n",
        "    binaryLabelDataset_transform.features, sensitive_attribute_index, axis=1\n",
        ")\n",
        "y_train = binaryLabelDataset_transform.labels.ravel()\n",
        "\n",
        "# Initialize model\n",
        "lr = LogisticRegression(solver=\"liblinear\", penalty=\"l2\", C=0.001)\n",
        "\n",
        "\n",
        "# Train model\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_train_pred = lr.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test the classifier\n",
        "\n",
        "Now, evaluate the performance of the trained classifier on the transformed test dataset by using `.predict()`.\n",
        "\n",
        "Make sure to repair (DI transform) the test dataset as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_prep = np.concatenate(\n",
        "    (\n",
        "        data_processor.transform(test_data).todense(),\n",
        "        test_data[[model_target]].values,\n",
        "        test_data[[sensitive_attribute]].values,\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "# Add column names and convert to a DataFrame\n",
        "test_prep_df = pd.DataFrame(\n",
        "    test_prep, columns=ft_names + [model_target] + [sensitive_attribute]\n",
        ")\n",
        "\n",
        "# Create a dataset construct for AIF360\n",
        "binaryLabelDataset_test = BinaryLabelDataset(\n",
        "    df=test_prep_df,\n",
        "    label_names=[\">50k\"],\n",
        "    protected_attribute_names=[\"RAC1P\"],\n",
        "    favorable_label=1.0,\n",
        "    unfavorable_label=0.0,\n",
        ")\n",
        "\n",
        "# Transform the test dataset\n",
        "test_repd = di_remover.fit_transform(binaryLabelDataset_test)\n",
        "\n",
        "# Get test data to validate the classifier\n",
        "X_test = np.delete(test_repd.features, sensitive_attribute_index, axis=1)\n",
        "y_test = test_repd.labels.ravel()\n",
        "\n",
        "# Use the fitted model to make predictions on the test dataset\n",
        "test_predictions = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <br>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">How can you calculate the accuracy for the test dataset?</p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To answer the question, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell for a knowledge check question\n",
        "question_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the previous notebooks, you might remember that the accuracy was similar.\n",
        "\n",
        "Now, check whether the outcomes are fairer too. For this, create a DataFrame that contains the predictions as well as the sensitive attribute and the true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame that contains predictions and the sensitive attribute\n",
        "di_df = pd.concat(\n",
        "    [\n",
        "        test_data.reset_index(drop=True)[[\"RAC1P\", \">50k\"]],\n",
        "        pd.Series(test_predictions, name=\"y_test_pred_di\"),\n",
        "    ],\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check the DI of the predictions, you need to create a `BinaryLabelDataset` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset construct for AIF360\n",
        "pred_binaryLabelDataset = BinaryLabelDataset(\n",
        "    df=di_df,\n",
        "    label_names=[\"y_test_pred_di\"],\n",
        "    protected_attribute_names=[\"RAC1P\"],\n",
        "    favorable_label=1.0,\n",
        "    unfavorable_label=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    BinaryLabelDatasetMetric(\n",
        "        pred_binaryLabelDataset,\n",
        "        unprivileged_groups=unpriv_group,\n",
        "        privileged_groups=priv_group,\n",
        "    ).disparate_impact()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is much lower than the values that you had originally (before the transformation). Remember that you want a value greater than 0.8 (and as close to 1 as possible).\n",
        "\n",
        "How good is this model in terms of outcomes for the individuals in the dataset?\n",
        "\n",
        "You can plot the distribution of targets and inspect whether the model works equally well for different subgroups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "sns.catplot(x=\"RAC1P\", kind=\"count\", hue=model_target, data=di_df, palette=\"husl\")\n",
        "plt.ylim(0, 2500)\n",
        "sns.catplot(x=\"RAC1P\", kind=\"count\", hue=\"y_test_pred_di\", data=di_df, palette=\"husl\")\n",
        "plt.ylim(0, 2500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Conclusion\n",
        "Similar to what you had in the previous model, you have underpredictions in the >50k class. Furthermore, for this particular dataset, the DI transformation did not have the desired effect because the disfavored group is still at a disadvantage (relative to ground truth). Compared to the previous notebook, the results are certainly more fair because group 6 has fewer positive outcomes, and group 8 has more positive outcomes (compared to a model that was trained without any intervention at all). However, this is still not ideal. Also, is this actually fair for group 6 now, which has fewer positive outcomes than in the ground truth?\n",
        "\n",
        "## Next lab\n",
        "In the next lab, you will learn about bias mitigation during model training and postprocessing by using reweighing and equalized odds."
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
